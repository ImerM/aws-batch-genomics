{
    "docs": [
        {
            "location": "/",
            "text": "Large scale genomics workflows on AWS\n\n\nWelcome! This tutorial walks through how to set up \nAmazon Web Services\n (\nAWS\n) products, such as \nAmazon S3\n, \nAWS Batch\n, etc., for running large scale genomics analyses. A typical genomics workflow is represented by the diagram below:\n\n\n\n\nSpecifically, we want to create a system that handles packaging applications, executing individual tasks, and orchastrating the data between tasks.\n\n\nPrerequisites\n\n\nWe make a few assumptions on your experience:\n\n\n\n\nYou are familiar with the Linux command line\n\n\nYou can use SSH to log into a Linux server\n\n\nYou have a working AWS account\n\n\nThat account is able to create a \nAWS Batch\n environment.\n\n\n\n\nIf you are completely new to AWS, we \nhighly recommend\n going through the following two \nAWS 10-Minute Tutorials\n.\n\n\n\n\nLaunch a Linux Virtual Machine\n - A tutorial which walks users through the process of starting a host on AWS, and configuring your own computer to connect over SSH.\n\n\nBatch upload files to the cloud\n - A tutorial on using the AWS Command Line Interface (CLI) to access Amazon S3.\n\n\n\n\nThe above tutorials will demonstrate the basics of AWS, as well as set up your development machine for working with AWS.\n\n\n\n\nTip\n\n\nWe \nstrongly\n recommend following the \nIAM Security Best Practices\nfor securing your root AWS account and IAM users.\n\n\n\n\nSetting up an AWS environment for genomics\n\n\nThere are several services at AWS that can be used for genomics. In this tutorial, we focus on \nAWS Batch\n. AWS Batch itself is built on top of other AWS services, such as \nAmazon EC2\n and \nAmazon ECS\n, and as such has a few requirements for escalated privileges to get started from scratch.\n\n\nFor example, you will need to be able to create some \nIAM Roles\n. AWS \nIdentity and Access Management (IAM)\n\nis a web service that helps you securely control access to AWS resources. You use IAM to control who is authenticated (signed in) and authorized (has permissions) to use resources.\n\n\nWe have provided some \nCloudFormation\n templates to make the initial environment setup less painful. We show how to use these in \nstep 2\n.\n\n\n\n\nNote\n\n\nIf you are using a institutional account, it is likely that it does not  have administrative privileges, such as the IAM  \nAdministratorAccess\n managed policy\n.\n\n\nIf this is the case, you will need to work with your account administrator to set up a AWS Batch environment for you. That means less work for you! Just point them at this guide, and hae them provide you with a \nAWS Batch Job Queue ARN\n, and a \nAmazon S3 Bucket\n that you can write results to. Move on to \nStep 3\n.\n\n\n\n\nAssuming that you have the proper permissions, you are ready for \nSetting up AWS Batch\n.",
            "title": "Home"
        },
        {
            "location": "/#large-scale-genomics-workflows-on-aws",
            "text": "Welcome! This tutorial walks through how to set up  Amazon Web Services  ( AWS ) products, such as  Amazon S3 ,  AWS Batch , etc., for running large scale genomics analyses. A typical genomics workflow is represented by the diagram below:   Specifically, we want to create a system that handles packaging applications, executing individual tasks, and orchastrating the data between tasks.",
            "title": "Large scale genomics workflows on AWS"
        },
        {
            "location": "/#prerequisites",
            "text": "We make a few assumptions on your experience:   You are familiar with the Linux command line  You can use SSH to log into a Linux server  You have a working AWS account  That account is able to create a  AWS Batch  environment.   If you are completely new to AWS, we  highly recommend  going through the following two  AWS 10-Minute Tutorials .   Launch a Linux Virtual Machine  - A tutorial which walks users through the process of starting a host on AWS, and configuring your own computer to connect over SSH.  Batch upload files to the cloud  - A tutorial on using the AWS Command Line Interface (CLI) to access Amazon S3.   The above tutorials will demonstrate the basics of AWS, as well as set up your development machine for working with AWS.   Tip  We  strongly  recommend following the  IAM Security Best Practices for securing your root AWS account and IAM users.",
            "title": "Prerequisites"
        },
        {
            "location": "/#setting-up-an-aws-environment-for-genomics",
            "text": "There are several services at AWS that can be used for genomics. In this tutorial, we focus on  AWS Batch . AWS Batch itself is built on top of other AWS services, such as  Amazon EC2  and  Amazon ECS , and as such has a few requirements for escalated privileges to get started from scratch.  For example, you will need to be able to create some  IAM Roles . AWS  Identity and Access Management (IAM) \nis a web service that helps you securely control access to AWS resources. You use IAM to control who is authenticated (signed in) and authorized (has permissions) to use resources.  We have provided some  CloudFormation  templates to make the initial environment setup less painful. We show how to use these in  step 2 .   Note  If you are using a institutional account, it is likely that it does not  have administrative privileges, such as the IAM   AdministratorAccess  managed policy .  If this is the case, you will need to work with your account administrator to set up a AWS Batch environment for you. That means less work for you! Just point them at this guide, and hae them provide you with a  AWS Batch Job Queue ARN , and a  Amazon S3 Bucket  that you can write results to. Move on to  Step 3 .   Assuming that you have the proper permissions, you are ready for  Setting up AWS Batch .",
            "title": "Setting up an AWS environment for genomics"
        },
        {
            "location": "/aws-batch/configure-aws-batch-start/",
            "text": "Introduction on AWS Batch for genomics workflows\n\n\nTL;DR\n\n\nAWS Batch provides a queue to send jobs to, and will manage the underlying compute necessary to run those jobs. For the impatient, skip to the next step \n\"Creating a custom AMI for genomics\"\n.\n\n\nThe detailed explaination\n\n\nAWS Batch\n is a managed service that helps you efficiently run batch computing workloads on the AWS Cloud. Users submit jobs to job queues, specifying the application to be run and their jobs\u2019 CPU and memory requirements. AWS Batch is responsible for launching the appropriate quantity and types of instances needed to run your jobs.\n\n\nAWS Batch removes the undifferentiated heavy lifting of configuring and managing compute infrastructure, allowing you to instead focus on your applications and users. This is demonstrated in the \nHow AWS Batch Works\n video.\n\n\nAWS Batch manages the following resources:\n\n\n\n\nJob Definitions\n\n\nJob Queues\n\n\nCompute Environments\n\n\n\n\nA \njob definition\n specifies how jobs are to be run\u2014for example, which Docker image to use for your job, how many vCPUs and how much memory is required, the IAM role to be used, and more.\n\n\nJobs are submitted to \njob queues\n where they reside until they can be scheduled to run on Amazon EC2 instances within a compute environment. An AWS account can have multiple job queues, each with varying priority. This gives you the ability to closely align the consumption of compute resources with your organizational requirements.\n\n\nCompute environments\n provision and manage your EC2 instances and other compute resources that are used to run your AWS Batch jobs. Job queues are mapped to one more compute environments and a given environment can also be mapped to one or more job queues. This many-to-many relationship is defined by the compute environment order and job queue priority properties.\n\n\nThe following diagram shows a general overview of how the AWS Batch resources interact.\n\n\n\n\nWe will be leveraging \nAWS CloudFormation\n, which allows developers and systems administrators to easily create and manage a collection of related AWS resources (called a CloudFormation stack) by provisioning and updating them in an orderly and predictable way.\n\n\nThe provided CloudFormation templates will create the necessary resource for AWS within your Amazon VPC.\n\n\n\n\nSpecifically, the templates will create:\n\n\n\n\nA new Amazon S3 bucket to write results to.\n\n\nA new Amazon S3 bucket to send log data to.\n\n\nA AWS Batch Compute Environment that utilizes \nEC2 Spot instances\n for cost-effective computing\n\n\nA AWS Batch Compute Environment that utilizes EC2 on-demand (e.g. \npublic pricing\n) instances for high-priority work that can't risk job interruptions or delays due to insufficient Spot capacity.\n\n\nA default AWS Batch Job Queue that utilizes the Spot compute environment first, but falls back to the on-demand compute environment if there is spare capacity already there.\n\n\nA high-priority AWS Batch Job Queue that leverages the on-demand and Spot CE's (in that order) and has higher priority than the default queue.\n\n\n\n\nHere is a conceptual diagram of the proposed architecture:\n\n\n\n\nNext Step: Setting up a custom AMI for genomics workflows\n\n\nGenomics is a data-heavy workload and requires some modification to the standard AWS Batch processing environment. In particular, we need to scale underlying instance storage that Tasks/Jobs run on top of to meet unpredictable runtime demands.\n\n\nCreate a custom AMI for genomics workloads",
            "title": "Getting Started"
        },
        {
            "location": "/aws-batch/configure-aws-batch-start/#introduction-on-aws-batch-for-genomics-workflows",
            "text": "",
            "title": "Introduction on AWS Batch for genomics workflows"
        },
        {
            "location": "/aws-batch/configure-aws-batch-start/#tldr",
            "text": "AWS Batch provides a queue to send jobs to, and will manage the underlying compute necessary to run those jobs. For the impatient, skip to the next step  \"Creating a custom AMI for genomics\" .",
            "title": "TL;DR"
        },
        {
            "location": "/aws-batch/configure-aws-batch-start/#the-detailed-explaination",
            "text": "AWS Batch  is a managed service that helps you efficiently run batch computing workloads on the AWS Cloud. Users submit jobs to job queues, specifying the application to be run and their jobs\u2019 CPU and memory requirements. AWS Batch is responsible for launching the appropriate quantity and types of instances needed to run your jobs.  AWS Batch removes the undifferentiated heavy lifting of configuring and managing compute infrastructure, allowing you to instead focus on your applications and users. This is demonstrated in the  How AWS Batch Works  video.  AWS Batch manages the following resources:   Job Definitions  Job Queues  Compute Environments   A  job definition  specifies how jobs are to be run\u2014for example, which Docker image to use for your job, how many vCPUs and how much memory is required, the IAM role to be used, and more.  Jobs are submitted to  job queues  where they reside until they can be scheduled to run on Amazon EC2 instances within a compute environment. An AWS account can have multiple job queues, each with varying priority. This gives you the ability to closely align the consumption of compute resources with your organizational requirements.  Compute environments  provision and manage your EC2 instances and other compute resources that are used to run your AWS Batch jobs. Job queues are mapped to one more compute environments and a given environment can also be mapped to one or more job queues. This many-to-many relationship is defined by the compute environment order and job queue priority properties.  The following diagram shows a general overview of how the AWS Batch resources interact.   We will be leveraging  AWS CloudFormation , which allows developers and systems administrators to easily create and manage a collection of related AWS resources (called a CloudFormation stack) by provisioning and updating them in an orderly and predictable way.  The provided CloudFormation templates will create the necessary resource for AWS within your Amazon VPC.   Specifically, the templates will create:   A new Amazon S3 bucket to write results to.  A new Amazon S3 bucket to send log data to.  A AWS Batch Compute Environment that utilizes  EC2 Spot instances  for cost-effective computing  A AWS Batch Compute Environment that utilizes EC2 on-demand (e.g.  public pricing ) instances for high-priority work that can't risk job interruptions or delays due to insufficient Spot capacity.  A default AWS Batch Job Queue that utilizes the Spot compute environment first, but falls back to the on-demand compute environment if there is spare capacity already there.  A high-priority AWS Batch Job Queue that leverages the on-demand and Spot CE's (in that order) and has higher priority than the default queue.   Here is a conceptual diagram of the proposed architecture:",
            "title": "The detailed explaination"
        },
        {
            "location": "/aws-batch/configure-aws-batch-start/#next-step-setting-up-a-custom-ami-for-genomics-workflows",
            "text": "Genomics is a data-heavy workload and requires some modification to the standard AWS Batch processing environment. In particular, we need to scale underlying instance storage that Tasks/Jobs run on top of to meet unpredictable runtime demands.  Create a custom AMI for genomics workloads",
            "title": "Next Step: Setting up a custom AMI for genomics workflows"
        },
        {
            "location": "/aws-batch/create-custom-ami/",
            "text": "Custom AWS Batch compute resources for genomics\n\n\nA default AWS Batch environment assumes that the storage available to the \nAmazon ECS-Optimized AMI\n meets the needs of most customers. Any other needs, such as the large scratch storage requirements noted above, or devices like GPUs, can be handled by providing AWS Batch with a custom \nCompute Resource AMI\n.\n\n\nGenomics is a data-heavy workload and requires some modification to the standard AWS Batch processing environment. In particular, we need the underlying instance storage that tasks (\nAWS Batch Jobs\n) run on top of to meet unpredictable runtime demands.\n\n\nWe have provided a script (see \nthe next section\n) that customizes the ECS-Optimized AMI to add a working directory that the Jobs will use to write data. That directory will be monitored by a process that inspects the free space available and adds more EBS volumes and expands the filesystem on the fly, like so:\n\n\n\n\nCreate a custom AMI\n\n\nWe have provided a Python script that sets up the above.\n\n\nThe script will:\n\n\n\n\nLaunch an EC2 instance from the ECS-Optimized AMI with a encrypted EBS volumes for the Docker containers  and scratch space\n\n\nAdjust the system settings to mount the scratch on instance start.\n\n\nInstall and configure a small service to monitor and automatically expand the scratch space by adding new EBS volume\n\n\nMake the necessary adjustments to the Amazon Elastic Container Service (ECS) agent to work with AWS Batch\n\n\nAdjust the network settings to allow for containers to query instance metadata for their Task IAM roles.\n\n\n\n\n# Download the script and install the requirements\ncurl -O https://raw.githubusercontent.com/aws-samples/aws-batch-genomics/master/src/custom-ami/create-custom-ami.py\npip install boto3\n\n# Run the script to see the help\npython create-custom-ami.py --help\n# Output:\n# No default VPC found. You must provide *both* VPC and Subnet IDs that are able to access public IP domains on CLI\n# usage: create-genomics-ami.py [-h] [--scratch_mount_point SCRATCH_MOUNT_POINT]\n#                               [--key-pair-name KEY_PAIR_NAME]\n#                               [--vpc-id VPC_ID] [--subnet-id SUBNET_ID]\n#                               [--security-group-id SECURITY_GROUP_ID]\n#                               [--terminate-instance] [--no-terminate-instance]\n\n\n\n\n\n\nNote\n\n\nIf you are unable to leverage this script, you likely don't have the permissions to work in this enviroment. Talk with your account administrator and show them this guide.\n\n\n\n\nThe \n--key-pair-name\n parameter defaults to \n\"genomics-ami\"\n. The script will create the key pair if it does not exist and write out a PEM file (\ngenomics-ami.pem\n) to the same directory as where you ran the script. If the key pair already exists, we assume that you know how to find it for your use.\n\n\nBy default, the script terminates the new instance. If you want to leave the instance up to SSH into and review, provice the \n--no-terminate-instance\n argument.\n\n\nMost new accounts have a \ndefault VPC\n, but if this is not the case, or if you want to leverage a non-default VPC, then supply \nboth\n the \n--vpc-id\n and \n--subnet-id\n parameters.\n\n\nThe script takes about 10 minutes to run, you may want to take a \n or \n  break at this point. Here is an example of the output from running the script (\nvalues for ID's have been changed\n):\n\n\nGetting the security group from name GenomicsAmiSG-subnet-123ab123\nSecurity Group GenomicsAmiSG-subnet-123ab123 does not exist. Creating.\nKey Pair genomics-ami-west2 does not exist. Creating.\nKey Pair PEM file written to  genomics-ami-west2.pem\nLaunching a new EC2 instance.\nWaiting on instance to have a IP...[ 111.222.111.222 ].\nWaiting on instance to pass health checks.................................instance available and healthy.\nMinting a new AMI...........................new AMI [ami-123abc123] created.\nTerminating instance...terminated.\n\nResources that were created on your behalf:\n\n    * EC2 Key Pair: genomics-ami-west2\n    * EC2 Security Group: sg-12ab1234\n    * EC2 Instance ID: i-01234abcde2134\n    * EC2 AMI ImageId: ami-123abc123\n\nTake note the returned EC2 AMI ImageId. We will use that for the AWS Batch setup.\n\n\n\n\n\nOnce the script completes, you have a new AMI ID to give to AWS Batch. Make a note of the AMI ID that was returned, we will need it for future sections. If you chose to not terminate the instance,  you can also SSH into the server to review the services. Be sure to terminate the instance after you are done. Here is an example using the AWS CLI.\n\n\naws ec2 terminate-instances --instance-ids i-01234abcde2134",
            "title": "Creating a custom AMI"
        },
        {
            "location": "/aws-batch/create-custom-ami/#custom-aws-batch-compute-resources-for-genomics",
            "text": "A default AWS Batch environment assumes that the storage available to the  Amazon ECS-Optimized AMI  meets the needs of most customers. Any other needs, such as the large scratch storage requirements noted above, or devices like GPUs, can be handled by providing AWS Batch with a custom  Compute Resource AMI .  Genomics is a data-heavy workload and requires some modification to the standard AWS Batch processing environment. In particular, we need the underlying instance storage that tasks ( AWS Batch Jobs ) run on top of to meet unpredictable runtime demands.  We have provided a script (see  the next section ) that customizes the ECS-Optimized AMI to add a working directory that the Jobs will use to write data. That directory will be monitored by a process that inspects the free space available and adds more EBS volumes and expands the filesystem on the fly, like so:",
            "title": "Custom AWS Batch compute resources for genomics"
        },
        {
            "location": "/aws-batch/create-custom-ami/#create-a-custom-ami",
            "text": "We have provided a Python script that sets up the above.  The script will:   Launch an EC2 instance from the ECS-Optimized AMI with a encrypted EBS volumes for the Docker containers  and scratch space  Adjust the system settings to mount the scratch on instance start.  Install and configure a small service to monitor and automatically expand the scratch space by adding new EBS volume  Make the necessary adjustments to the Amazon Elastic Container Service (ECS) agent to work with AWS Batch  Adjust the network settings to allow for containers to query instance metadata for their Task IAM roles.   # Download the script and install the requirements\ncurl -O https://raw.githubusercontent.com/aws-samples/aws-batch-genomics/master/src/custom-ami/create-custom-ami.py\npip install boto3\n\n# Run the script to see the help\npython create-custom-ami.py --help\n# Output:\n# No default VPC found. You must provide *both* VPC and Subnet IDs that are able to access public IP domains on CLI\n# usage: create-genomics-ami.py [-h] [--scratch_mount_point SCRATCH_MOUNT_POINT]\n#                               [--key-pair-name KEY_PAIR_NAME]\n#                               [--vpc-id VPC_ID] [--subnet-id SUBNET_ID]\n#                               [--security-group-id SECURITY_GROUP_ID]\n#                               [--terminate-instance] [--no-terminate-instance]   Note  If you are unable to leverage this script, you likely don't have the permissions to work in this enviroment. Talk with your account administrator and show them this guide.   The  --key-pair-name  parameter defaults to  \"genomics-ami\" . The script will create the key pair if it does not exist and write out a PEM file ( genomics-ami.pem ) to the same directory as where you ran the script. If the key pair already exists, we assume that you know how to find it for your use.  By default, the script terminates the new instance. If you want to leave the instance up to SSH into and review, provice the  --no-terminate-instance  argument.  Most new accounts have a  default VPC , but if this is not the case, or if you want to leverage a non-default VPC, then supply  both  the  --vpc-id  and  --subnet-id  parameters.  The script takes about 10 minutes to run, you may want to take a   or    break at this point. Here is an example of the output from running the script ( values for ID's have been changed ):  Getting the security group from name GenomicsAmiSG-subnet-123ab123\nSecurity Group GenomicsAmiSG-subnet-123ab123 does not exist. Creating.\nKey Pair genomics-ami-west2 does not exist. Creating.\nKey Pair PEM file written to  genomics-ami-west2.pem\nLaunching a new EC2 instance.\nWaiting on instance to have a IP...[ 111.222.111.222 ].\nWaiting on instance to pass health checks.................................instance available and healthy.\nMinting a new AMI...........................new AMI [ami-123abc123] created.\nTerminating instance...terminated.\n\nResources that were created on your behalf:\n\n    * EC2 Key Pair: genomics-ami-west2\n    * EC2 Security Group: sg-12ab1234\n    * EC2 Instance ID: i-01234abcde2134\n    * EC2 AMI ImageId: ami-123abc123\n\nTake note the returned EC2 AMI ImageId. We will use that for the AWS Batch setup.  Once the script completes, you have a new AMI ID to give to AWS Batch. Make a note of the AMI ID that was returned, we will need it for future sections. If you chose to not terminate the instance,  you can also SSH into the server to review the services. Be sure to terminate the instance after you are done. Here is an example using the AWS CLI.  aws ec2 terminate-instances --instance-ids i-01234abcde2134",
            "title": "Create a custom AMI"
        },
        {
            "location": "/aws-batch/configure-aws-batch-cfn/",
            "text": "Launching the CloudFormation stacks\n\n\nThe links below provide a fully configured AWS environment for setting up AWS Batch for genomics workflows. In all cases, you will need the AMI ID for the AWS Batch Compute Resource AMI that you created using the \n\"Create a Custom AMI\"\n guide.\n\n\n\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nSource\n\n\nLaunch Stack\n\n\n\n\n\n\n\n\n\n\nFull Stack\n\n\nLaunch a full AWS environment, including a new VPC across 2 Availability Zones, IAM policies and roles, Amazon S3 buckets for data and logging, and AWS Batch Job Queue and Compute Environments. \nYou must provide a custom AMI ID\n.\n\n\n\n\n\n\n\n\n\n\nExisting VPC\n\n\nCreate AWS Batch Job Queues and Compute Environments, a secure Amazon S3 bucket, and IAM policies and roles within an existing VPC. \nYou must provide custom AMI ID, VPC ID, and subnet IDs\n.\n\n\n\n\n\n\n\n\n\n\n\n\nThe individual components from above are available as stand-alone CloudFormation templates as well, in case you need to have another individual with elevated privileges to execute one of them. They are in order of dependency, and you will need to provide output values from one template to the dependent templates.\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nSource\n\n\nLaunch Stack\n\n\n\n\n\n\n\n\n\n\nAmazon VPC\n\n\nThe AWS reference VPC deployment from the \nModular and Scalable VPC Architecure\n guide. Great for deploying genomics workflows to.\n\n\n\n\n\n\n\n\n\n\nAmazon S3\n\n\nCreates a secure Amazon S3 bucket to read from and write results to.\n\n\n\n\n\n\n\n\n\n\nAmazon IAM\n\n\nCreate the necessary IAM Roles. This is useful to hand to someone with the right permissions to create these on your behalf. \nYou will need to provide a S3 bucket name\n.\n\n\n\n\n\n\n\n\n\n\nAWS Batch\n\n\nCreates AWS Batch Job Queues and Compute Environments. You will need to provide the details on IAM roles and instance profiles, and the IDs for a VPC and subnets.\n\n\n\n\n\n\n\n\n\n\n\n\nFull Stack\n guided walk-through\n\n\nHere we provide a walkthrough launching the \nFull Stack\n CloudFormation template.\n\n\nStep 1. Launch the template\n\n\nStep 2. Choose a good name for the stack\n\n\nStep 3. Fill out the parameters\n\n\nStep 4. Take a break\n\n\nStep 5. Get the AWS Batch Job Queue\n\n\nNow that the CloudFormation stack is launched, note the output default AWS Batch Job Queue name and ARN. You will need these when configuring a workflow system, such as GenomicsWorkflow or Nextflow, to use AWS Batch as a backend for task distribution.",
            "title": "Launch the CloudFormation stacks"
        },
        {
            "location": "/aws-batch/configure-aws-batch-cfn/#launching-the-cloudformation-stacks",
            "text": "The links below provide a fully configured AWS environment for setting up AWS Batch for genomics workflows. In all cases, you will need the AMI ID for the AWS Batch Compute Resource AMI that you created using the  \"Create a Custom AMI\"  guide.      Name  Description  Source  Launch Stack      Full Stack  Launch a full AWS environment, including a new VPC across 2 Availability Zones, IAM policies and roles, Amazon S3 buckets for data and logging, and AWS Batch Job Queue and Compute Environments.  You must provide a custom AMI ID .      Existing VPC  Create AWS Batch Job Queues and Compute Environments, a secure Amazon S3 bucket, and IAM policies and roles within an existing VPC.  You must provide custom AMI ID, VPC ID, and subnet IDs .       The individual components from above are available as stand-alone CloudFormation templates as well, in case you need to have another individual with elevated privileges to execute one of them. They are in order of dependency, and you will need to provide output values from one template to the dependent templates.     Name  Description  Source  Launch Stack      Amazon VPC  The AWS reference VPC deployment from the  Modular and Scalable VPC Architecure  guide. Great for deploying genomics workflows to.      Amazon S3  Creates a secure Amazon S3 bucket to read from and write results to.      Amazon IAM  Create the necessary IAM Roles. This is useful to hand to someone with the right permissions to create these on your behalf.  You will need to provide a S3 bucket name .      AWS Batch  Creates AWS Batch Job Queues and Compute Environments. You will need to provide the details on IAM roles and instance profiles, and the IDs for a VPC and subnets.",
            "title": "Launching the CloudFormation stacks"
        },
        {
            "location": "/aws-batch/configure-aws-batch-cfn/#full-stack-guided-walk-through",
            "text": "Here we provide a walkthrough launching the  Full Stack  CloudFormation template.",
            "title": "Full Stack guided walk-through"
        },
        {
            "location": "/aws-batch/configure-aws-batch-cfn/#step-1-launch-the-template",
            "text": "",
            "title": "Step 1. Launch the template"
        },
        {
            "location": "/aws-batch/configure-aws-batch-cfn/#step-2-choose-a-good-name-for-the-stack",
            "text": "",
            "title": "Step 2. Choose a good name for the stack"
        },
        {
            "location": "/aws-batch/configure-aws-batch-cfn/#step-3-fill-out-the-parameters",
            "text": "",
            "title": "Step 3. Fill out the parameters"
        },
        {
            "location": "/aws-batch/configure-aws-batch-cfn/#step-4-take-a-break",
            "text": "",
            "title": "Step 4. Take a break"
        },
        {
            "location": "/aws-batch/configure-aws-batch-cfn/#step-5-get-the-aws-batch-job-queue",
            "text": "Now that the CloudFormation stack is launched, note the output default AWS Batch Job Queue name and ARN. You will need these when configuring a workflow system, such as GenomicsWorkflow or Nextflow, to use AWS Batch as a backend for task distribution.",
            "title": "Step 5. Get the AWS Batch Job Queue"
        },
        {
            "location": "/aws-batch/notes-vpc/",
            "text": "While you can use an existing default VPC to implement deployment of your genomics environment, we strongly recommend utilizing a VPC with private subnets for the processing sensitive data with AWS Batch. Doing so will restrict access to the instances from the internet, and help meet security and compliance requirements, such as \ndbGaP\n.\n\n\nWe recommend the use of the AWS Quickstart reference deployment for a \nModular and Scalable VPC Architecture\n. This Quick Start provides a networking foundation for AWS Cloud infrastructures. It deploys an Amazon Virtual Private Cloud (Amazon VPC) according to AWS best practices and guidelines.\n\n\nThe Amazon VPC reference architecture includes public and private subnets. The first set of private subnets share the default network access control list (ACL) from the Amazon VPC, and a second, optional set of private subnets include dedicated custom network ACLs per subnet. The Quick Start divides the Amazon VPC address space in a predictable manner across multiple Availability Zones, and deploys either NAT instances or NAT gateways, depending on the AWS Region you deploy the Quick Start in.\n\n\nFor architectural details, best practices, step-by-step instructions, and customization options, see the\n\ndeployment guide\n.\n\n\n\n\nTip\n\n\nYou may also want to review the \nHIPAA on AWS Enterprise Accelerator\n and the \nAWS Biotech Blueprint\n for additional security best practices such as:\n\n\n\n\nBasic AWS Identity and Access Management (IAM) configuration with custom (IAM) policies, with associated groups, roles, and instance profiles\n\n\nStandard, external-facing Amazon Virtual Private Cloud (Amazon VPC) Multi-AZ architecture with separate subnets for different application tiers and private (back-end) subnets for application and database\n\n\nAmazon Simple Storage Service (Amazon S3) buckets for encrypted web content, logging, and backup data\n\n\nStandard Amazon VPC security groups for Amazon Elastic Compute Cloud (Amazon EC2) instances and load balancers used in the sample application stack\n\n\nA secured bastion login host to facilitate command-line Secure Shell (SSH) access to Amazon EC2 instances for troubleshooting and systems administration activities\n\n\nLogging, monitoring, and alerts using AWS CloudTrail, Amazon CloudWatch, and AWS Config rules",
            "title": "Notes on Amazon VPC"
        },
        {
            "location": "/aws-batch/notes-iam/",
            "text": "",
            "title": "Notes on IAM Permissions"
        },
        {
            "location": "/genomics-workflows-intro/",
            "text": "Genomics Workflows\n\n\nNow that we have a way to execute individual tasks via AWS Batch, we turn to orchastration of complete workflows. A typical genomics workflow is represented by the diagram below:\n\n\n\n\nIn order to process data, we will need to handle the cases for serial and parallel task execution, and retry logic when a task fails.\n\n\nThe domain logic for workflows should live outside of the code for any individual task. There are a couple of systems that researchers can use to define and execute repeatable data analysis pipelines on AWS Batch:\n\n\n\n\nNative AWS services\n such as AWS Lambda and AWS Step Functions.\n\n\n\n\n\n\n\n\n\n\nFollow one of the links above to configure a full genomics workflows computing environment on AWS.\n\n\n\n\nNote\n\n\nWe do not cover other systems such as Galaxy or SnakeMake as they do not support AWS Batch (yet).",
            "title": "Introduction"
        },
        {
            "location": "/genomics-workflows-intro/#genomics-workflows",
            "text": "Now that we have a way to execute individual tasks via AWS Batch, we turn to orchastration of complete workflows. A typical genomics workflow is represented by the diagram below:   In order to process data, we will need to handle the cases for serial and parallel task execution, and retry logic when a task fails.  The domain logic for workflows should live outside of the code for any individual task. There are a couple of systems that researchers can use to define and execute repeatable data analysis pipelines on AWS Batch:   Native AWS services  such as AWS Lambda and AWS Step Functions.     Follow one of the links above to configure a full genomics workflows computing environment on AWS.   Note  We do not cover other systems such as Galaxy or SnakeMake as they do not support AWS Batch (yet).",
            "title": "Genomics Workflows"
        },
        {
            "location": "/step-functions/intro-step-functions/",
            "text": "Leveraging AWS-native services for genomics workflows\n\n\nA system for defining and executing genomics workflows can be subdivided into three main service areas.\n\n\n\n\nServices for managing applications and data.\n\n\nSerivces for individual task execution.\n\n\nOrchastration services that manage the execution of processes and the flow of data between tasks.\n\n\n\n\nThe following diagram illustrates a reference AWS architecture for these three service layers.\n\n\n\n\nFor \ndata\n, we leverage Amazon S3 as the source of truth. All input and output data are staged to/from S3 as part of the task. \nApplications\n are deployed as Docker containers, and Amazon Elastic Container Registry is used to hold our Docker container images.\n\n\nFor \ntask scheduling and execution\n, we rely on AWS Batch.\n\n\nFor \norchastration between tasks\n, the combination of \nAWS Step Functions\n and \nAWS Lambda\n provide a robust mechanism to orchastrate complex workflows by interacting directly with AWS Batch.\n\n\nIn the next few sections, we will cover an example of how to impement a workflow system that leverages AWS Step Functions with AWS Lambda.\n\n\nRequirements for AWS Batch Jobs\n\n\nAWS Batch does not make assumptions on the structure and requirements that Jobs take with respect to inputs and outputs. Batch Jobs may take data streams, files, or only parameters as input, and produce the same variaty for output, inclusive of files, metadata changes, updates to databases, etc. The space is so varied, Batch assumes that each application handles their input/output requirements.\n\n\nA common pattern for bioinformatics, however, is that files such as images or genomic sequence data are the inputs to some process, and outputs of that process are also files. Many bioinformatics tools have also been developed to run in traditional Linux-based compute clusters with shared filesystems, and are not necessarily optimized for cloud computing. In all the set of common requirements are for genomics on AWS Batch are:\n\n\nContainer placement\n\n\nTo make your workflow as flexible as possible, each job should run independently. As a result, you cannot necessarily guarantee that different jobs in the same overall workflow run on the same instance. Using S3 as the location to exchange data between containers enables you to decouple storage of your intermediate files from compute.\n\n\nMultitenancy\n\n\nMultiple container jobs may run concurrently on the same instance. In these situations, it\u2019s essential that your job writes to a unique subdirectory.\n\n\nCleanup\n\n\nAs your jobs complete and write the output back to S3, it is a good idea to delete the scratch data on your instance generated by that job. This allows you to optimize for cost by reusing EC2 instances if there are jobs remaining in the queue, rather than terminating the EC2 instances.\n\n\nJob management using a container\n\n\nSince these requirements are commonplace, other bioinformatics-focused workflow systems, such as \nNextflow\n, have support for data staging, etc. For a system based on AWS Step Functions and AWS Lambda, we will leverage a Docker container to handle the data staging and multitenancy requirements. The container will launch the application of interest, such as SAMTools, as a sibling container process.\n\n\nSeparating the application's container from the AWS Batch requirements of running an application allows for leveraging 3rd party tools easier, and also allows for management of the functionality of the workflow system to be independent from the application's runtime needs. The diagram below represents the interaction between AWS Batch, Amazon S3, the job management container (\nBatchJobRunner\n), and the application's container (\nsamtools\n):\n\n\n\n\nAs you can see, the job management container can handle the requirements for most bioinformatics applications. You can also extend this container beyond the basics to perform other functions, such as sending real-time resource utilization and other runtime information for the application to a central monitoring service.\n\n\nNext we cover how to build a basic job management container",
            "title": "Introduction and requirements"
        },
        {
            "location": "/step-functions/intro-step-functions/#leveraging-aws-native-services-for-genomics-workflows",
            "text": "A system for defining and executing genomics workflows can be subdivided into three main service areas.   Services for managing applications and data.  Serivces for individual task execution.  Orchastration services that manage the execution of processes and the flow of data between tasks.   The following diagram illustrates a reference AWS architecture for these three service layers.   For  data , we leverage Amazon S3 as the source of truth. All input and output data are staged to/from S3 as part of the task.  Applications  are deployed as Docker containers, and Amazon Elastic Container Registry is used to hold our Docker container images.  For  task scheduling and execution , we rely on AWS Batch.  For  orchastration between tasks , the combination of  AWS Step Functions  and  AWS Lambda  provide a robust mechanism to orchastrate complex workflows by interacting directly with AWS Batch.  In the next few sections, we will cover an example of how to impement a workflow system that leverages AWS Step Functions with AWS Lambda.",
            "title": "Leveraging AWS-native services for genomics workflows"
        },
        {
            "location": "/step-functions/intro-step-functions/#requirements-for-aws-batch-jobs",
            "text": "AWS Batch does not make assumptions on the structure and requirements that Jobs take with respect to inputs and outputs. Batch Jobs may take data streams, files, or only parameters as input, and produce the same variaty for output, inclusive of files, metadata changes, updates to databases, etc. The space is so varied, Batch assumes that each application handles their input/output requirements.  A common pattern for bioinformatics, however, is that files such as images or genomic sequence data are the inputs to some process, and outputs of that process are also files. Many bioinformatics tools have also been developed to run in traditional Linux-based compute clusters with shared filesystems, and are not necessarily optimized for cloud computing. In all the set of common requirements are for genomics on AWS Batch are:",
            "title": "Requirements for AWS Batch Jobs"
        },
        {
            "location": "/step-functions/intro-step-functions/#container-placement",
            "text": "To make your workflow as flexible as possible, each job should run independently. As a result, you cannot necessarily guarantee that different jobs in the same overall workflow run on the same instance. Using S3 as the location to exchange data between containers enables you to decouple storage of your intermediate files from compute.",
            "title": "Container placement"
        },
        {
            "location": "/step-functions/intro-step-functions/#multitenancy",
            "text": "Multiple container jobs may run concurrently on the same instance. In these situations, it\u2019s essential that your job writes to a unique subdirectory.",
            "title": "Multitenancy"
        },
        {
            "location": "/step-functions/intro-step-functions/#cleanup",
            "text": "As your jobs complete and write the output back to S3, it is a good idea to delete the scratch data on your instance generated by that job. This allows you to optimize for cost by reusing EC2 instances if there are jobs remaining in the queue, rather than terminating the EC2 instances.",
            "title": "Cleanup"
        },
        {
            "location": "/step-functions/intro-step-functions/#job-management-using-a-container",
            "text": "Since these requirements are commonplace, other bioinformatics-focused workflow systems, such as  Nextflow , have support for data staging, etc. For a system based on AWS Step Functions and AWS Lambda, we will leverage a Docker container to handle the data staging and multitenancy requirements. The container will launch the application of interest, such as SAMTools, as a sibling container process.  Separating the application's container from the AWS Batch requirements of running an application allows for leveraging 3rd party tools easier, and also allows for management of the functionality of the workflow system to be independent from the application's runtime needs. The diagram below represents the interaction between AWS Batch, Amazon S3, the job management container ( BatchJobRunner ), and the application's container ( samtools ):   As you can see, the job management container can handle the requirements for most bioinformatics applications. You can also extend this container beyond the basics to perform other functions, such as sending real-time resource utilization and other runtime information for the application to a central monitoring service.  Next we cover how to build a basic job management container",
            "title": "Job management using a container"
        },
        {
            "location": "/step-functions/create-batchjobrunner-container/",
            "text": "Creating the job management container\n\n\nFor a job management container (\nBatchJobRunner\n) we will need to install the necessary tooling to support temporary runtime directories, downloads/uploads from S3, and running sibling containers that actually execute the desired process.\n\n\nFROM amazonlinux:2\n\nRUN yum update -y && \\\n    yum install -y awscli docker jq unzip && \\\n    yum clean all\n\nRUN cd /opt && \\\n    curl -o batch-job-runner.tgz https://github.com/delagoya/batch-task-runner/archive/master.zip && \\\n    tar -xzf batch-job-runner.tgz && rm batch-job-runner.tgz\n\nENTRYPOINT [\"/opt/batch-job-runner.sh\"]\n\n\n\n\nThe above Dockerfile defines the source operating system (\nAmazon Linux 2\n and installs all of the necessary packages and dependencies for the job manager script (\nbatch-job-runner.sh\n), and then sets that script as the container's entry point. Assuming your working directory has a file with the above contents, create the container like so:\n\n\ndocker build .",
            "title": "Creating a job management container"
        },
        {
            "location": "/step-functions/create-batchjobrunner-container/#creating-the-job-management-container",
            "text": "For a job management container ( BatchJobRunner ) we will need to install the necessary tooling to support temporary runtime directories, downloads/uploads from S3, and running sibling containers that actually execute the desired process.  FROM amazonlinux:2\n\nRUN yum update -y && \\\n    yum install -y awscli docker jq unzip && \\\n    yum clean all\n\nRUN cd /opt && \\\n    curl -o batch-job-runner.tgz https://github.com/delagoya/batch-task-runner/archive/master.zip && \\\n    tar -xzf batch-job-runner.tgz && rm batch-job-runner.tgz\n\nENTRYPOINT [\"/opt/batch-job-runner.sh\"]  The above Dockerfile defines the source operating system ( Amazon Linux 2  and installs all of the necessary packages and dependencies for the job manager script ( batch-job-runner.sh ), and then sets that script as the container's entry point. Assuming your working directory has a file with the above contents, create the container like so:  docker build .",
            "title": "Creating the job management container"
        },
        {
            "location": "/step-functions/create-example-sfl-workflow/",
            "text": "",
            "title": "An example workflow"
        },
        {
            "location": "/cromwell/cromwell-aws-batch/",
            "text": "",
            "title": "Cromwell"
        },
        {
            "location": "/nextflow/nextflow-aws-batch/",
            "text": "Nextflow.io on AWS Batch\n\n\nConfiguring Nextflow to leverage AWS Batch only requires two items:\n\n\n\n\nThat \neither\n the pipeline processes \nor\n the \nnextflow.config\n file define the \ncontainer\n directive\n\n\nThat the \nprocess.executor\n property is set to \n'awsbatch'\n in the \nnextflow.config\n file\n\n\n\n\nHere is an example config file that has two\n\n\n\n\n\n\n\nPre-configured EC2 instance\n\n\nThe following CloudFormation template will launch a EC2 instance pre-configured for using Nextflow.\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nSource\n\n\nLaunch Stack\n\n\n\n\n\n\n\n\n\n\nLogin Host Nextflow\n\n\nLaunch a EC2 instance preconfigured for Nextflow and AWS Batch.",
            "title": "Nextflow"
        },
        {
            "location": "/nextflow/nextflow-aws-batch/#nextflowio-on-aws-batch",
            "text": "Configuring Nextflow to leverage AWS Batch only requires two items:   That  either  the pipeline processes  or  the  nextflow.config  file define the  container  directive  That the  process.executor  property is set to  'awsbatch'  in the  nextflow.config  file   Here is an example config file that has two",
            "title": "Nextflow.io on AWS Batch"
        },
        {
            "location": "/nextflow/nextflow-aws-batch/#pre-configured-ec2-instance",
            "text": "The following CloudFormation template will launch a EC2 instance pre-configured for using Nextflow.     Name  Description  Source  Launch Stack      Login Host Nextflow  Launch a EC2 instance preconfigured for Nextflow and AWS Batch.",
            "title": "Pre-configured EC2 instance"
        }
    ]
}